"""
MODULE: FEATURE ENGINEERING & FILTERING PIPELINE
X·ª≠ l√Ω sau khi EDA ho√†n th√†nh:
1. L·ªçc downtime (d·ª±a tr√™n EDA insights)
2. L·ªçc bot/DDoS (d·ª±a tr√™n error_rate threshold)
3. T·∫°o Lag features (24h v√† 7d cycles)
4. T·∫°o Time features (hour, day_of_week, is_weekend)
"""
import yaml
import logging
from pathlib import Path
import pandas as pd
import numpy as np

# ============================================================
# CONFIGURATION
# ============================================================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def load_config():
    """T·∫£i c·∫•u h√¨nh t·ª´ file YAML."""
    base_dir = Path(__file__).resolve().parent.parent
    config_path = base_dir / "config" / "config.yaml"
    with open(config_path, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)

CONFIG = load_config()

# ============================================================
# CLASS: FEATURE ENGINEER
# ============================================================
class FeatureEngineer:
    """
    X·ª≠ l√Ω l·ªçc d·ªØ li·ªáu v√† t·∫°o features sau khi EDA ph√°t hi·ªán insights.
    """
    
    def __init__(self, config):
        self.config = config
       
        self.downtime_start = pd.Timestamp("1995-07-28 13:35:00")
        self.downtime_end = pd.Timestamp("1995-08-03 04:37:00")
        self.bot_error_threshold = config.get('analysis', {}).get('bot_error_threshold', 0.8)
    
    def filter_downtime(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        L·ªçc b·ªè giai ƒëo·∫°n downtime (t·ª´ EDA insights).
        
        Downtime: T·ª´ ng√†y 28/07 13:35:00 -> 03/08 04:36:13
        """
        logger.info(f"üîç ƒêang l·ªçc downtime: {self.downtime_start} ‚Üí {self.downtime_end}")
        
        before_count = len(df)
        
        # L·ªçc b·ªè kho·∫£ng downtime
        df_filtered = df[~((df['ds'] >= self.downtime_start) & (df['ds'] <= self.downtime_end))].copy()
        
        removed_count = before_count - len(df_filtered)
        logger.info(f"  ‚úì ƒê√£ lo·∫°i b·ªè {removed_count:,} observation trong kho·∫£ng downtime")
        
        return df_filtered
    
    def filter_bot_ddos(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        L·ªçc b·ªè c√°c observation c√≥ t·ªâ l·ªá l·ªói b·∫•t th∆∞·ªùng (ph√°t hi·ªán Bot/DDoS).
        """
        logger.info(f"üîç ƒêang l·ªçc bot/DDoS (error_rate >= {self.bot_error_threshold})")
        
        before_count = len(df)
        df_filtered = df[df['error_rate'] < self.bot_error_threshold].copy()
        removed_count = before_count - len(df_filtered)
        
        logger.info(f"  ‚úì ƒê√£ lo·∫°i b·ªè {removed_count:,} observation (noise)")
        
        return df_filtered

    def filter_intensity(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        L·ªçc c√°c gi√° tr·ªã intensity kh√¥ng h·ª£p l·ªá ho·∫∑c ngo·∫°i lai.

        - Lo·∫°i NaN trong c·ªôt `intensity`
        - Lo·∫°i gi√° tr·ªã √¢m
        - Lo·∫°i c√°c ngo·∫°i lai tr√™n percentile cao (m·∫∑c ƒë·ªãnh 99.5%)
        """
        logger.info("üîç ƒêang l·ªçc intensity (NaN, √¢m, ngo·∫°i lai)")

        if 'intensity' not in df.columns:
            logger.warning("  ‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y c·ªôt 'intensity' ‚Äî b·ªè qua b∆∞·ªõc l·ªçc intensity")
            return df

        before = len(df)

        # lo·∫°i NaN v√† gi√° tr·ªã √¢m
        df_filtered = df.dropna(subset=['intensity']).copy()
        df_filtered = df_filtered[df_filtered['intensity'] >= 0]

        # lo·∫°i ngo·∫°i lai ph√≠a tr√™n (upper percentile)
        try:
            upper_pct = 0.995
            upper_val = df_filtered['intensity'].quantile(upper_pct)
            outliers = df_filtered[df_filtered['intensity'] > upper_val]
            if len(outliers) > 0:
                logger.info(f"  ‚úì Lo·∫°i {len(outliers):,} observation > {int(upper_pct*100)}th percentile (>{upper_val:.2f})")
                df_filtered = df_filtered[df_filtered['intensity'] <= upper_val]
        except Exception:
            logger.exception("  ‚ö†Ô∏è Kh√¥ng th·ªÉ t√≠nh percentile intensity ‚Äî b·ªè qua l·ªçc ngo·∫°i lai")

        removed = before - len(df_filtered)
        logger.info(f"  ‚úì ƒê√£ lo·∫°i b·ªè {removed:,} observation (intensity)")

        return df_filtered

    
    
    def create_lag_features(self, df: pd.DataFrame, interval: str) -> pd.DataFrame:
        """
        T·∫°o Lag features cho 24h v√† 7d cycles.
        
        Args:
            df: DataFrame sau l·ªçc
            interval: '1min', '5min', ho·∫∑c '15min'
        """
        logger.info(f"üîß T·∫°o Lag features cho khung {interval}...")
        
        if interval == '1min':
            df['lag_1440'] = df['intensity'].shift(1440)      # 24h (1440 √ó 1min)
            df['lag_10080'] = df['intensity'].shift(10080)     # 7d (10080 √ó 1min)
            logger.info(f"  ‚úì lag_1440 (24h), lag_10080 (7d)")
        
        elif interval == '5min':
            df['lag_288'] = df['intensity'].shift(288)         # 24h (288 √ó 5min)
            df['lag_2016'] = df['intensity'].shift(2016)       # 7d (2016 √ó 5min)
            logger.info(f"  ‚úì lag_288 (24h), lag_2016 (7d)")
        
        elif interval == '15min':
            df['lag_96'] = df['intensity'].shift(96)           # 24h (96 √ó 15min)
            df['lag_672'] = df['intensity'].shift(672)         # 7d (672 √ó 15min)
            logger.info(f"  ‚úì lag_96 (24h), lag_672 (7d)")
        
        return df
    
    def create_time_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        T·∫°o Time features t·ª´ timestamp.
        H·ªó tr·ª£ c·∫£ c·ªôt 'ds' v√† 'timestamp'.
        """
        logger.info("üîß T·∫°o Time features (hour, day_of_week, is_weekend)...")
        
        # X√°c ƒë·ªãnh c·ªôt th·ªùi gian
        time_col = None
        if 'ds' in df.columns:
            time_col = 'ds'
        elif 'timestamp' in df.columns:
            time_col = 'timestamp'
        else:
            logger.warning("  ‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y c·ªôt timestamp/ds")
            return df
        
        # ƒê·∫£m b·∫£o l√† datetime
        df[time_col] = pd.to_datetime(df[time_col])
        
        # Hour: 0-23
        df['hour'] = df[time_col].dt.hour
        
        # Day of week: 0-6 (Monday=0, Sunday=6)
        df['day_of_week'] = df[time_col].dt.dayofweek
        
        # Is weekend: 1 n·∫øu Saturday-Sunday, 0 n·∫øu weekday
        df['is_weekend'] = (df[time_col].dt.dayofweek >= 5).astype(int)
        
        logger.info(f"  ‚úì hour, day_of_week, is_weekend")
        
        return df
    
    def process_single_interval(self, input_path: Path, output_path: Path, interval: str, file_type: str):
        """
        X·ª≠ l√Ω m·ªôt khung th·ªùi gian duy nh·∫•t.
        
        Args:
            input_path: ƒê∆∞·ªùng d·∫´n file RAW (t·ª´ preprocessing)
            output_path: ƒê∆∞·ªùng d·∫´n file output (sau feature engineering)
            interval: '1min', '5min', '15min'
            file_type: 'train' ho·∫∑c 'test'
        """
        logger.info(f"‚ñ∂ ƒêang x·ª≠ l√Ω {file_type.upper()} - {interval}...")

        if not input_path.exists():
            logger.error(f"  ‚ùå File kh√¥ng t·ªìn t·∫°i: {input_path}")
            return False

        # Load main input (raw, ch∆∞a l·ªçc) and mark source
        df_main = pd.read_csv(input_path)
        df_main['ds'] = pd.to_datetime(df_main['ds'])
        df_main = df_main.copy()
        df_main['__source__'] = 'train' if file_type == 'train' else 'test'
        logger.info(f"  Loaded {len(df_main):,} observations from input")

        combined = df_main

        # n·∫øu l√† test, load th√™m train ƒë·ªÉ t·∫°o lag features ch√≠nh x√°c
        train_input = input_path.parent / f"processed_train_{interval}.csv"
        if file_type == 'test' and train_input.exists():
            train_df_raw = pd.read_csv(train_input)
            train_df_raw['ds'] = pd.to_datetime(train_df_raw['ds'])
            train_df_raw = train_df_raw.copy()
            train_df_raw['__source__'] = 'train'
            combined = pd.concat([train_df_raw, df_main], ignore_index=True)
            logger.info(f"  Loaded train ({len(train_df_raw):,}) and concatenated for lag calculation")

        # S·∫Øp x·∫øp theo th·ªùi gian tr∆∞·ªõc khi t·∫°o lag features
        combined = combined.sort_values('ds').reset_index(drop=True)
        
        # NOTE: Lag features ƒë∆∞·ª£c t√≠nh t·ª´ test data th·∫≠t (khi file_type='test')
        # ƒêi·ªÅu n√†y c√≥ v·∫ª nh∆∞ data leakage, NH∆ØNG:
        # - ƒê√¢y l√† ·ªü feature engineering time (offline), kh√¥ng ph·∫£i prediction time
        # - Khi predict (online), XGBoostForecaster/LSTMForecaster.predict() s·∫Ω c·∫≠p nh·∫≠t
        #   lag features t·ª´ PREDICTIONS (kh√¥ng ph·∫£i test data th·∫≠t) via roll-forward mechanism
        # - Xem models.py predict() method ƒë·ªÉ hi·ªÉu chi ti·∫øt
        combined = self.create_lag_features(combined, interval)
        combined = self.create_time_features(combined)

        # Sau khi t·∫°o lag features, l·ªçc d·ªØ li·ªáu
        combined = self.filter_downtime(combined)
        combined = self.filter_bot_ddos(combined)
        combined = self.filter_intensity(combined)

        # T√°ch l·∫°i train/test v√† xu·∫•t
        if file_type == 'test':
            out_df = combined[combined['__source__'] == 'test'].copy().reset_index(drop=True)
            logger.info(f"  Extracted test data: {len(out_df):,} observations")
        else:
            out_df = combined[combined['__source__'] == 'train'].copy().reset_index(drop=True)
            out_df = out_df.dropna().reset_index(drop=True)
            logger.info(f"  ‚úì Sau x·ª≠ l√Ω: {len(out_df):,} observations")

        if '__source__' in out_df.columns:
            out_df = out_df.drop(columns=['__source__'])

        out_df.to_csv(output_path, index=False)
        logger.info(f"  ‚úÖ Xu·∫•t: {output_path}")

        return True

# ============================================================
# MAIN EXECUTION
# ============================================================
def run_feature_engineering(file_type='train'):
    """
    Ch·∫°y feature engineering cho train ho·∫∑c test.
    """
    logger.info(f"\n{'='*60}")
    logger.info(f"FEATURE ENGINEERING: {file_type.upper()}")
    logger.info(f"{'='*60}")

    BASE_DIR = Path(__file__).resolve().parent.parent
    DATA_DIR = BASE_DIR / "data"

    engineer = FeatureEngineer(CONFIG)
    intervals = CONFIG['processing']['intervals']

    for interval in intervals:
        input_file = f"processed_{file_type}_{interval}.csv"
        output_file = f"prepared_{file_type}_{interval}.csv"

        input_path = DATA_DIR / input_file
        output_path = DATA_DIR / output_file

        try:
            success = engineer.process_single_interval(
                input_path,
                output_path,
                interval,
                file_type
            )
            if not success:
                logger.error(f"  Failed to process {interval}")
        except Exception as e:
            logger.error(f"  ‚ùå L·ªói: {e}")
            import traceback
            traceback.print_exc()


if __name__ == "__main__":
    print("\n" + "="*70)
    print(" FEATURE ENGINEERING & FILTERING PIPELINE")
    print("="*70)

    try:
        run_feature_engineering(file_type='train')
        run_feature_engineering(file_type='test')

        print("\n" + "*"*70)
        print(" ‚úÖ HO√ÄN TH√ÄNH FEATURE ENGINEERING!")
        print(" üìÅ C√°c file ƒë√£ t·∫°o:")
        print("    - prepared_train_1min.csv, prepared_train_5min.csv, prepared_train_15min.csv")
        print("    - prepared_test_1min.csv, prepared_test_5min.csv, prepared_test_15min.csv")
        print("*"*70 + "\n")

    except Exception as e:
        logger.error(f"‚ùå L·ªñI H·ªÜ TH·ªêNG: {e}")
        import traceback
        traceback.print_exc()

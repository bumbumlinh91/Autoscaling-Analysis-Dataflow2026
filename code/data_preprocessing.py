"""
MODULE: DATA PREPROCESSING PIPELINE """
import re
import yaml
import logging
from pathlib import Path
from datetime import datetime
import pandas as pd
import numpy as np

# 1. CONFIGURATION 
# Thi·∫øt l·∫≠p Logging ƒë·ªÉ theo d√µi ti·∫øn ƒë·ªô
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# C√°c h·∫±ng s·ªë quan tr·ªçng
def load_config():
    # ƒê∆∞·ªùng d·∫´n t∆∞∆°ng ƒë·ªëi ƒë·∫øn config.yaml
    base_dir = Path(__file__).resolve().parent.parent
    config_path = base_dir / "config" / "config.yaml"
    with open(config_path, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)
# L·∫•y th√¥ng tin downtime t·ª´ config
CONFIG = load_config()
downtime_cfg = CONFIG.get('processing', {}).get('downtime', {})
DOWNTIME_START = pd.Timestamp(downtime_cfg.get('start'))
DOWNTIME_END = pd.Timestamp(downtime_cfg.get('end'))
logger.info(f"üìÖ C·∫•u h√¨nh Downtime: {DOWNTIME_START} -> {DOWNTIME_END}")


# Report cho host v√† content
def export_top_hosts(df, output_dir):
    """Xu·∫•t b√°o c√°o Top Host ph·ª•c v·ª• qu√° tr√¨nh EDA."""
    print("üïµÔ∏è‚Äç‚ôÇÔ∏è [REPORT] ƒêang tr√≠ch xu·∫•t Top 20 Hosts...")
    try:
        if 'host' not in df.columns: return

        # ƒê·∫øm t·∫ßn su·∫•t
        top_hosts = df['host'].value_counts().head(20).reset_index()
        top_hosts.columns = ['host', 'request_count']

        # T√≠nh % ƒë√≥ng g√≥p
        total = len(df)
        top_hosts['percentage'] = (top_hosts['request_count'] / total) * 100

        # L∆∞u file
        save_path = output_dir / "top_hosts_report.csv"
        top_hosts.to_csv(save_path, index=False)
        print(f"‚úÖ ƒê√£ l∆∞u b√°o c√°o Host t·∫°i: {save_path}")
    except Exception as e:
        print(f"‚ö†Ô∏è L·ªói xu·∫•t Top Host: {e}")


def export_content_report(df, output_dir):
    """Xu·∫•t b√°o c√°o Lo·∫°i n·ªôi dung ƒë·ªÉ hi·ªÉu h√†nh vi user."""
    print("üïµÔ∏è‚Äç‚ôÇÔ∏è [REPORT] ƒêang ph√¢n t√≠ch n·ªôi dung Request...")

    def extract_extension(req_str):
        try:
            # L·∫•y chu·ªói gi·ªØa "GET " v√† " HTTP" 
            parts = req_str.split()
            if len(parts) > 1:
                url = parts[1]
                if '.' in url:
                    return url.split('.')[-1].lower()
            return 'unknown'
        except:
            return 'error'

    try:
        if 'request' not in df.columns: return

        # X·ª≠ l√Ω v√† ƒë·∫øm t·∫ßn su·∫•t
        temp_series = df['request'].apply(extract_extension)
        content_stats = temp_series.value_counts().head(15).reset_index()
        content_stats.columns = ['file_type', 'count']

        save_path = output_dir / "content_report.csv"
        content_stats.to_csv(save_path, index=False)
        print(f"‚úÖ ƒê√£ l∆∞u b√°o c√°o Content t·∫°i: {save_path}")
    except Exception as e:
        print(f"‚ö†Ô∏è L·ªói xu·∫•t Content Report: {e}")

# 2. CLASS: PARSER

class LogParser:
    """
    Class ch·ªãu tr√°ch nhi·ªám ƒë·ªçc file th√¥ v√† chuy·ªÉn th√†nh DataFrame.
    S·ª≠ d·ª•ng k·ªπ thu·∫≠t ƒë·ªçc t·ª´ng d√≤ng ƒë·ªÉ tr√°nh tr√†n RAM.
    """
    
    # Regex Pattern: Compiled m·ªôt l·∫ßn ƒë·ªÉ t·ªëi ∆∞u hi·ªáu su·∫•t
    # B√≥c t√°ch: Host, Timestamp, Request, Status, Bytes
    LOG_PATTERN = re.compile(
        r'(?P<host>\S+) \S+ \S+ '
        r'\[(?P<timestamp>[^\]]+)\] '
        r'"(?P<request>[^"]*)" '
        r'(?P<status>\d{3}) '
        r'(?P<bytes>\S+)'
    )

    def parse_line(self, line):
        """X·ª≠ l√Ω t·ª´ng d√≤ng log m·ªôt."""
        match = self.LOG_PATTERN.search(line)
        if not match:
            return None
        
        data = match.groupdict()
        
        # X·ª≠ l√Ω Bytes: K√Ω t·ª± '-' nghƒ©a l√† 0 bytes
        data['bytes'] = 0 if data['bytes'] == '-' else int(data['bytes'])
        
        return data

    def load_data(self, file_path: Path) -> pd.DataFrame:
        """
        ƒê·ªçc file log v√† tr·∫£ v·ªÅ DataFrame th√¥.
        Args:
            file_path (Path): ƒê∆∞·ªùng d·∫´n ƒë·∫øn file data.
        """
        records = []
        logger.info(f"ƒêang ƒë·ªçc d·ªØ li·ªáu t·ª´: {file_path}")
        
        try:
            # D√πng 'utf-8' v√† 'errors=ignore' ƒë·ªÉ x·ª≠ l√Ω c√°c k√Ω t·ª± l·∫° trong log c≈©
            with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
                for i, line in enumerate(f):
                    parsed = self.parse_line(line)
                    if parsed:
                        records.append(parsed)
                    
                    # Log ti·∫øn ƒë·ªô m·ªói 200,000 d√≤ng ƒë·ªÉ bi·∫øt code kh√¥ng b·ªã treo
                    if i > 0 and i % 200000 == 0:
                        logger.info(f"ƒê√£ x·ª≠ l√Ω {i} d√≤ng...")
                        
            logger.info(f"Ho√†n t·∫•t ƒë·ªçc file. T·ªïng s·ªë d√≤ng h·ª£p l·ªá: {len(records)}")
            return pd.DataFrame(records)
            
        except FileNotFoundError:
            logger.error(f"L·ªñI: Kh√¥ng t√¨m th·∫•y file t·∫°i {file_path}")
            raise


# 3. CLASS: PROCESSOR 

class DataProcessor:
    """
    Class ch·ªãu tr√°ch nhi·ªám l√†m s·∫°ch, chu·∫©n h√≥a v√† gom nh√≥m (Aggregation).
    """
    
    def clean_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:
        """Chu·∫©n h√≥a ki·ªÉu d·ªØ li·ªáu."""
        logger.info("ƒêang chu·∫©n h√≥a d·ªØ li·ªáu v√† x·ª≠ l√Ω m√∫i gi·ªù...")
        
        # Chuy·ªÉn ƒë·ªïi Timestamp c√≥ m√∫i gi·ªù
        df['timestamp'] = pd.to_datetime(
            df['timestamp'], 
            format='%d/%b/%Y:%H:%M:%S %z', 
            errors='coerce'
        )
        
        # B·ªè th√¥ng tin m√∫i gi·ªù ƒë·ªÉ so s√°nh d·ªÖ d√†ng
        df['timestamp'] = df['timestamp'].dt.tz_localize(None)
       
        # √âp ki·ªÉu s·ªë
        df['status'] = df['status'].astype(int)
        df['bytes'] = df['bytes'].astype(float) 
        
        # S·∫Øp x·∫øp theo th·ªùi gian 
        df = df.sort_values('timestamp').reset_index(drop=True)
        
        return df

    def aggregate_data(self, df: pd.DataFrame, window='5min') -> pd.DataFrame:
        """
        Gom nh√≥m d·ªØ li·ªáu theo khung th·ªùi gian (Resampling).
        ƒê√¢y l√† b∆∞·ªõc chu·∫©n b·ªã data cho Model Prophet/XGBoost.
        """
        logger.info(f"ƒêang gom nh√≥m d·ªØ li·ªáu theo khung {window}...")
        
        # Set timestamp l√†m index ƒë·ªÉ resample
        df_indexed = df.set_index('timestamp')
        
        # Logic Aggregation:
        # - hits: ƒê·∫øm t·ªïng s·ªë y√™u c·∫ßu
        # - bytes: T√≠nh t·ªïng v√† trung b√¨nh bƒÉng th√¥ng
        # - error_4xx: Ph√°t hi·ªán truy c·∫≠p r√°c ho·∫∑c link h·ªèng 
        # - error_5xx: Theo d√µi t√¨nh tr·∫°ng qu√° t·∫£i c·ªßa m√°y ch·ªß 
        agg_df = df_indexed.resample(window).agg({
            'request': 'count',                 # T·ªïng l∆∞·ª£t truy c·∫≠p 
            'bytes': ['sum', 'mean'],           # T·ªïng v√† trung b√¨nh bƒÉng th√¥ng
            'status': [
                ('error_4xx', lambda x: ((x >= 400) & (x < 500)).sum()), # L·ªói do kh√°ch h√†ng/Bot
                ('error_5xx', lambda x: (x >= 500).sum())                # L·ªói do h·ªá th·ªëng qu√° t·∫£i
            ]
        })
        
        # L√†m ph·∫≥ng MultiIndex th√†nh c·ªôt ƒë∆°n
        agg_df.columns = ['_'.join(col).strip() for col in agg_df.columns.values]
        agg_df.rename(columns={
            'request_count': 'y',               # Bi·∫øn m·ª•c ti√™u cho d·ª± b√°o l∆∞u l∆∞·ª£ng
            'status_error_4xx': 'error_4xx',
            'status_error_5xx': 'error_5xx',
            'bytes_sum': 'total_bytes',
            'bytes_mean': 'avg_bytes'
        }, inplace=True)
        
        # Reset ch·ªâ m·ª•c v√† chu·∫©n h√≥a t√™n c·ªôt th·ªùi gian sang 'ds'
        agg_df = agg_df.reset_index().rename(columns={'timestamp': 'ds'})
        
       # --- C√°c bi·∫øn n√¢ng cao ---
        # 1. T·ªâ l·ªá l·ªói: Ph·∫£n √°nh ƒë·ªô ·ªïn ƒë·ªãnh v√† s·ª©c kh·ªèe c·ªßa h·ªá th·ªëng
        agg_df['error_rate'] = (agg_df['error_4xx'] + agg_df['error_5xx']) / (agg_df['y'] + 1e-8)
        
        # 2. C∆∞·ªùng ƒë·ªô t·∫£i: Ch·ªâ s·ªë t·∫£i th·ª±c t·∫ø d·ª±a tr√™n l∆∞u l∆∞·ª£ng v√† bƒÉng th√¥ng
        # S·ª≠ d·ª•ng tr·ªçng s·ªë t·ª´ c·∫•u h√¨nh ƒë·ªÉ ph·∫£n √°nh √°p l·ª±c l√™n t√†i nguy√™n ph·∫ßn c·ª©ng
        weight = CONFIG.get('analysis', {}).get('resource_weight', 1.0)
        agg_df['intensity'] = agg_df['y'] * agg_df['avg_bytes'] * weight

        # 3. Ph√¢n lo·∫°i tr·∫°ng th√°i h·ªá th·ªëng
        # G·∫Øn nh√£n c√°c giai ƒëo·∫°n x·∫£y ra s·ª± c·ªë d·ª±a tr√™n khung th·ªùi gian c·∫•u h√¨nh
        agg_df['is_downtime'] = ((agg_df['ds'] >= DOWNTIME_START) & (agg_df['ds'] <= DOWNTIME_END)).astype(int)
        
        # X·ª≠ l√Ω c√°c gi√° tr·ªã thi·∫øu b·∫±ng ph∆∞∆°ng ph√°p ƒëi·ªÅn s·ªë 0 ƒë·ªÉ ƒë·∫£m b·∫£o t√≠nh li√™n t·ª•c c·ªßa d·ªØ li·ªáu
        return agg_df.fillna(0)


# 4. MAIN EXECUTION

def run_full_pipeline(file_type='train'):
    """
    H√†m Wrapper ch·∫°y m·ªôt l·∫ßn, xu·∫•t ra c·∫£ 3 khung th·ªùi gian: 1m, 5m, 15m.
    """
    # Thi·∫øt l·∫≠p ƒë∆∞·ªùng d·∫´n t∆∞∆°ng ƒë·ªëi 
    BASE_DIR = Path(__file__).resolve().parent.parent 
    DATA_DIR = BASE_DIR / "data"
    filename = CONFIG['paths']['train_file'] if file_type == 'train' else CONFIG['paths']['test_file']
    file_path = (BASE_DIR / CONFIG['paths']['input_dir']) / filename
    
    # Kh·ªüi t·∫°o
    parser = LogParser()
    processor = DataProcessor()
    
    # Ch·∫°y Pipeline (Ch·ªâ load v√† clean 1 l·∫ßn duy nh·∫•t ƒë·ªÉ ti·∫øt ki·ªám RAM)
    raw_df = parser.load_data(file_path)
    clean_df = processor.clean_dataframe(raw_df)

    if file_type == 'train':
        print(f"üìä ƒêang t·∫°o b√°o c√°o chi ti·∫øt (Host & Content) cho t·∫≠p Train...")
        # G·ªçi 2 h√†m ƒë√£ khai b√°o
        export_top_hosts(clean_df, DATA_DIR)
        export_content_report(clean_df, DATA_DIR)

    # Aggregate cho c·∫£ 3 khung th·ªùi gian
    intervals = CONFIG['processing']['intervals']
    processed_package = {}
    
    for interval in intervals:
        logger.info(f"--- ƒêang v·∫≠n h√†nh Pipeline cho khung th·ªùi gian: {interval} ---")
        
        # T·∫°o d·ªØ li·ªáu g·ªôp 
        agg_df = processor.aggregate_data(clean_df, window=interval)
        
        # Ph√¢n lo·∫°i v√† l·ªçc Downtime
        if DOWNTIME_START and DOWNTIME_END:
            # T·∫°o mask: True l√† gi·ªØ l·∫°i, False l√† x√≥a
            mask_valid = ~((agg_df['ds'] >= DOWNTIME_START) & (agg_df['ds'] <= DOWNTIME_END))
            
            dropped_rows = len(agg_df) - mask_valid.sum()
            
            if dropped_rows > 0:
                agg_df = agg_df[mask_valid] # C·∫≠p nh·∫≠t tr·ª±c ti·∫øp v√†o agg_df
                logger.warning(f"‚öîÔ∏è [DOWNTIME] ƒê√£ x√≥a bay {dropped_rows} d√≤ng r√°c trong khung {interval}.")
            else:
                logger.info("‚ÑπÔ∏è Kh√¥ng c√≥ d√≤ng n√†o trong v√πng Downtime ƒë·ªÉ x√≥a.")

        # T·∫°o b·ªô l·ªçc Bot d·ª±a tr√™n ng∆∞·ª°ng l·ªói
        threshold = CONFIG.get('analysis', {}).get('bot_error_threshold', 0.8)
        clean_agg_df = agg_df[agg_df['error_rate'] < threshold].copy()
        
        # Xu·∫•t CSV
        output_name = f"processed_{file_type}_{interval}.csv"
        clean_agg_df.to_csv(DATA_DIR / output_name, index=False)
        
        logger.info(f"‚úÖ ƒê√£ xu·∫•t file: {output_name} (ƒê√£ l·ªçc Downtime & Bot)")
        processed_package[interval] = clean_agg_df
        
    return processed_package

if __name__ == "__main__":
    print("\n" + "="*50)
    print(" H·ªÜ TH·ªêNG X·ª¨ L√ù D·ªÆ LI·ªÜU AUTOSCALING ")
    print("="*50)
    
    # T·ªáp d·ªØ li·ªáu c·∫ßn x·ª≠ l√Ω
    data_types = ['train', 'test']
    
    try:
        for dtype in data_types:
            print(f"\n>>> ƒêANG B·∫ÆT ƒê·∫¶U X·ª¨ L√ù T·∫¨P: {dtype.upper()}")
            run_full_pipeline(file_type=dtype)
        
        print("\n" + "*"*50)
        print(" CH√öC M·ª™NG: TO√ÄN B·ªò PIPELINE ƒê√É HO√ÄN TH√ÄNH! ")
        print(" - D·ªØ li·ªáu TRAIN: S·∫µn s√†ng ƒë·ªÉ hu·∫•n luy·ªán model. ")
        print(" - D·ªØ li·ªáu TEST : S·∫µn s√†ng ƒë·ªÉ ki·ªÉm th·ª≠ v√† EDA. ")
        print(" T·∫•t c·∫£ file CSV ƒë√£ n·∫±m g·ªçn trong th∆∞ m·ª•c 'data/'. ")
        print("*"*50)
        
    except Exception as e:
        # B√°o l·ªói 
        logger.error(f"L·ªñI H·ªÜ TH·ªêNG TRONG QU√Å TR√åNH X·ª¨ L√ù: {e}")

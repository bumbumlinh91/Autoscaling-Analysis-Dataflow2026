"""
SCRIPT: EVALUATION RUNNER
--------------------------------------------
M√¥ t·∫£:
    K·ªãch b·∫£n ƒë√°nh gi√° hi·ªáu nƒÉng m√¥ h√¨nh.
    T·ª± ƒë·ªông load d·ªØ li·ªáu test, model, scaler.
    T√≠nh to√°n c√°c ch·ªâ s·ªë ƒë√°nh gi√° v√† xu·∫•t b·∫£ng x·∫øp h·∫°ng.
"""
import sys
import yaml
import logging
import joblib
import torch
import pandas as pd
import numpy as np
from pathlib import Path
from sklearn.metrics import mean_squared_error, mean_absolute_error

# --- [FIX 1: SETUP ƒê∆Ø·ªúNG D·∫™N G·ªêC] ---
PROJECT_ROOT = Path(__file__).resolve().parent.parent
sys.path.append(str(PROJECT_ROOT))

# --- [FIX 2: IMPORT T·ª™ SRC] ---
from src.models import ProphetForecaster, XGBoostForecaster, LSTMForecaster

# Setup Logging
logging.basicConfig(level=logging.INFO, format='%(message)s')
logger = logging.getLogger()
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# [Optional] Hack module
sys.modules['models'] = sys.modules['src.models']

class Evaluator:
    def __init__(self):
        self.project_root = PROJECT_ROOT
        
        # ƒê∆∞·ªùng d·∫´n (L∆∞u √Ω: Train l∆∞u ·ªü ƒë√¢u th√¨ Evaluate ph·∫£i ƒë·ªçc ·ªü ƒë√≥)
        # N·∫øu train l∆∞u ·ªü 'models', h√£y s·ª≠a d√≤ng d∆∞·ªõi th√†nh 'models'
        self.models_dir = self.project_root / "saved_models"
        self.data_dir = self.project_root / "data"
        self.results_dir = self.project_root / "results"
        self.results_dir.mkdir(parents=True, exist_ok=True)
        self.target_col = 'intensity'
        
        # Load Config
        config_path = self.project_root / "config/config.yaml"
        try:
            with open(config_path, "r", encoding="utf-8") as f:
                self.config = yaml.safe_load(f)
            self.intervals = self.config['processing'].get('intervals', ['15min'])
        except Exception as e:
            print(f"‚ö†Ô∏è D√πng default intervals do l·ªói config: {e}")
            self.intervals = ['15min']
        
        print(f"üìã Danh s√°ch ƒë√°nh gi√°: {self.intervals}")
        print(f"üìÇ Th∆∞ m·ª•c Model: {self.models_dir}")

    def calculate_metrics(self, y_true, y_pred, interval):
        y_pred = np.maximum(y_pred, 0)
        
        # C·∫Øt ƒë·ªô d√†i
        min_len = min(len(y_true), len(y_pred))
        y_true = y_true[-min_len:]
        y_pred = y_pred[-min_len:]

        # WARM-UP CUT
        if '1min' in interval: cut = 1440
        elif '5min' in interval: cut = 288
        elif '15min' in interval: cut = 96
        else: cut = 0
        
        if len(y_true) > cut:
            y_true_s = y_true[cut:]
            y_pred_s = y_pred[cut:]
        else:
            y_true_s = y_true
            y_pred_s = y_pred
        
        mse = mean_squared_error(y_true_s, y_pred_s)
        rmse = np.sqrt(mse)
        mae = mean_absolute_error(y_true_s, y_pred_s)
        
        mask = y_true_s > 0
        if np.sum(mask) > 0:
            mape = np.mean(np.abs((y_true_s[mask] - y_pred_s[mask]) / y_true_s[mask])) * 100
        else:
            mape = 0.0 
            
        return rmse, mse, mae, mape

    def load_test_data(self, interval):
        path = self.data_dir / f"prepared_test_{interval}.csv"
        if not path.exists():
             path = self.data_dir / f"processed_test_{interval}.csv"
        
        if not path.exists():
            return None
        
        df = pd.read_csv(path)
        if 'ds' in df.columns:
            df['ds'] = pd.to_datetime(df['ds'])
            df = df.sort_values('ds').reset_index(drop=True)
        return df

    def run(self):
        leaderboard = []
        print(f"\n{'='*60}")
        print(f"üöÄ B·∫ÆT ƒê·∫¶U ƒê√ÅNH GI√Å (AUTO FIX FEATURE MISMATCH)")
        print(f"{'='*60}")

        for interval in self.intervals:
            print(f"\nüìÇ Interval: {interval}")
            
            # 1. Load Data
            df_test = self.load_test_data(interval)
            if df_test is None: 
                print(f"   ‚ùå Kh√¥ng t√¨m th·∫•y data test t·∫°i {self.data_dir}")
                continue
            
            # 2. Dynamic Feature Selection (L·∫•y t·∫•t c·∫£ feature ti·ªÅm nƒÉng)
            exclude_cols = ['ds', 'timestamp', self.target_col, 'y']
            feature_cols = [c for c in df_test.columns if c not in exclude_cols]
            
            y_true = df_test[self.target_col].values 
            df_preds = pd.DataFrame({'ds': df_test['ds'], 'Actual': y_true})

            # --- MODEL 1: PROPHET ---
            try:
                model_path = self.models_dir / f"prophet_{interval}.pkl"
                if model_path.exists():
                    model_p = joblib.load(model_path)
                    pred_p = model_p.predict(df_test)[-len(y_true):]
                    
                    rmse, mse, mae, mape = self.calculate_metrics(y_true, pred_p, interval)
                    leaderboard.append({'Interval': interval, 'Model': 'Prophet', 'RMSE': rmse, 'MSE': mse, 'MAE': mae, 'MAPE (%)': mape})
                    df_preds['Prophet'] = pred_p
                    print(f"   ‚úÖ Prophet: MAPE={mape:.2f}%")
                else:
                    print(f"   ‚ö†Ô∏è Thi·∫øu Prophet: {model_path.name}")
            except Exception as e: print(f"   ‚ùå Prophet Error: {e}")

            # --- LOAD SCALER & AUTO FIX MISMATCH ---
            scaler_X_path = self.models_dir / f"scaler_X_{interval}.pkl"
            scaler_y_path = self.models_dir / f"scaler_y_{interval}.pkl"
            
            if not scaler_X_path.exists() or not scaler_y_path.exists():
                print(f"   ‚ùå Thi·∫øu file Scaler. B·ªè qua XGB/LSTM.")
                continue

            try:
                scaler_X = joblib.load(scaler_X_path)
                scaler_y = joblib.load(scaler_y_path)
                
                # [FIX TH√îNG MINH] Ki·ªÉm tra s·ªë l∆∞·ª£ng feature
                expected_features = scaler_X.n_features_in_
                current_features = len(feature_cols)
                
                if current_features != expected_features:
                    print(f"   ‚ö†Ô∏è C·∫£nh b√°o: Scaler c·∫ßn {expected_features} c·ªôt, nh∆∞ng t√¨m th·∫•y {current_features} c·ªôt.")
                    # N·∫øu thi·∫øu 12 vs 20 -> C√≥ kh·∫£ nƒÉng 12 c·ªôt ƒë·∫ßu l√† 12 c·ªôt c≈©
                    # Ta s·∫Ω th·ª≠ c·∫Øt l·∫•y ƒë√∫ng s·ªë l∆∞·ª£ng c·ªôt ƒë·∫ßu ti√™n
                    print(f"   üîß ƒêang t·ª± ƒë·ªông c·∫Øt {expected_features} c·ªôt ƒë·∫ßu ti√™n ƒë·ªÉ kh·ªõp...")
                    X_vals = df_test[feature_cols].values[:, :expected_features]
                else:
                    X_vals = df_test[feature_cols].values

                X_scaled = scaler_X.transform(X_vals)
                
            except Exception as e:
                print(f"   ‚ùå L·ªói Scaler kh√¥ng th·ªÉ c·ª©u ch·ªØa: {e}")
                print("   üëâ H√£y ch·∫°y l·∫°i 'python -m scripts.train' ƒë·ªÉ ƒë·ªìng b·ªô model.")
                continue

            # --- MODEL 2: XGBOOST ---
            try:
                model_path = self.models_dir / f"xgboost_{interval}.pkl"
                if model_path.exists():
                    model_xgb = joblib.load(model_path)
                    pred_sc = model_xgb.predict(X_scaled)
                    pred_xgb = scaler_y.inverse_transform(pred_sc.reshape(-1, 1)).flatten()
                    
                    rmse, mse, mae, mape = self.calculate_metrics(y_true, pred_xgb, interval)
                    leaderboard.append({'Interval': interval, 'Model': 'XGBoost', 'RMSE': rmse, 'MSE': mse, 'MAE': mae, 'MAPE (%)': mape})
                    df_preds['XGBoost'] = pred_xgb
                    print(f"   ‚úÖ XGBoost: MAPE={mape:.2f}%")
            except Exception as e: print(f"   ‚ùå XGBoost Error: {e}")

            # --- MODEL 3: LSTM ---
            try:
                model_path = self.models_dir / f"lstm_{interval}.pth"
                if model_path.exists():
                    input_dim = X_scaled.shape[1]
                    forecaster = LSTMForecaster(self.config, input_dim)
                    forecaster.model.load_state_dict(torch.load(model_path, map_location=DEVICE))
                    forecaster.model.eval()
                    
                    n_lags = self.config['models']['lstm'].get('n_lags', 30)
                    if len(X_scaled) > n_lags:
                        X_seq = np.array([X_scaled[i:i+n_lags] for i in range(len(X_scaled)-n_lags)])
                        inp = torch.from_numpy(X_seq).float().to(DEVICE)
                        with torch.no_grad():
                            p = forecaster.model(inp).cpu().numpy().flatten()
                        
                        pred_lstm = scaler_y.inverse_transform(p.reshape(-1, 1)).flatten()
                        
                        y_trim = y_true[n_lags:]
                        rmse, mse, mae, mape = self.calculate_metrics(y_trim, pred_lstm, interval)
                        
                        leaderboard.append({'Interval': interval, 'Model': 'LSTM', 'RMSE': rmse, 'MSE': mse, 'MAE': mae, 'MAPE (%)': mape})
                        df_preds['LSTM'] = np.concatenate([[np.nan]*n_lags, pred_lstm])
                        print(f"   ‚úÖ LSTM: MAPE={mape:.2f}%")
            except Exception as e: print(f"   ‚ùå LSTM Error: {e}")

            # L∆∞u k·∫øt qu·∫£
            out_csv = self.results_dir / f"predictions_{interval}.csv"
            df_preds.to_csv(out_csv, index=False)
            print(f"   üíæ Saved CSV: {out_csv.name}")

        # --- XU·∫§T B·∫¢NG K·∫æT QU·∫¢ ---
        if leaderboard:
            df = pd.DataFrame(leaderboard).sort_values(by=['Interval', 'RMSE'])
            df = df[['Interval', 'Model', 'RMSE', 'MSE', 'MAE', 'MAPE (%)']]
            
            print(f"\n{'='*60}")
            print(f"üèÜ B·∫¢NG X·∫æP H·∫†NG K·∫æT QU·∫¢ (LEADERBOARD)")
            print(f"{'='*60}")
            print(df.to_string(index=False))
            
            df.to_csv(self.results_dir / "final_leaderboard.csv", index=False)
        else:
            print("\n‚ö†Ô∏è Kh√¥ng c√≥ k·∫øt qu·∫£ n√†o. H√£y ki·ªÉm tra l·∫°i th∆∞ m·ª•c saved_models!")

if __name__ == "__main__":
    Evaluator().run()
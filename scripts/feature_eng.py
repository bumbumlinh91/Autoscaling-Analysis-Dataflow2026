"""
SCRIPT: FEATURE ENGINEERING RUNNER
----------------------------------
"""
import sys
import yaml
import logging
import pandas as pd
from pathlib import Path

# Setup Path Ä‘á»ƒ import Ä‘Æ°á»£c src
PROJECT_ROOT = Path(__file__).resolve().parent.parent
sys.path.append(str(PROJECT_ROOT))

# Import Class tá»« src 
from src.feature_engineering import FeatureEngineeringPipeline

logging.basicConfig(level=logging.INFO, format='%(asctime)s | %(levelname)s | %(message)s')
logger = logging.getLogger(__name__)

def load_config():
    p = PROJECT_ROOT / "config/config.yaml"
    if p.exists():
        with open(p, "r", encoding="utf-8") as f: return yaml.safe_load(f)
    sys.exit("âŒ Config not found")

def main():
    config = load_config()
    data_dir = PROJECT_ROOT / "data"
    pipeline = FeatureEngineeringPipeline(config)
    
    target_intervals = ['1min', '5min', '15min']
    train_context_cache = {} # Cache 8 ngÃ y cuá»‘i cá»§a train Ä‘á»ƒ dÃ¹ng cho test

    print(f"\nğŸš€ START FEATURE ENGINEERING \n")

    # 1. Xá»­ lÃ½ TRAIN trÆ°á»›c
    for interval in target_intervals:
        input_path = data_dir / f"processed_train_{interval}.csv"
        if not input_path.exists(): continue
        
        logger.info(f"â–¶ Processing TRAIN: {interval}")
        df = pd.read_csv(input_path)
        df['ds'] = pd.to_datetime(df['ds'])
        
        # Gá»i hÃ m xá»­ lÃ½ tá»« src
        df_prepared = pipeline.process(df, interval)
        
        # LÆ°u context cho bÆ°á»›c Test
        cutoff = df_prepared['ds'].max() - pd.Timedelta(days=8)
        train_context_cache[interval] = df_prepared[df_prepared['ds'] > cutoff][['ds', 'intensity']].copy()
        
        # LÆ°u file
        out_path = data_dir / f"prepared_train_{interval}.csv"
        df_prepared.to_csv(out_path, index=False)
        logger.info(f"   ğŸ’¾ Saved: {out_path.name} (Cols: {len(df_prepared.columns)})")

    # 2. Xá»­ lÃ½ TEST sau
    for interval in target_intervals:
        input_path = data_dir / f"processed_test_{interval}.csv"
        if not input_path.exists(): continue
        
        logger.info(f"â–¶ Processing TEST: {interval}")
        df = pd.read_csv(input_path)
        df['ds'] = pd.to_datetime(df['ds'])
        
        # Láº¥y context tÆ°Æ¡ng á»©ng
        context = train_context_cache.get(interval)
        
        # Gá»i hÃ m xá»­ lÃ½
        df_prepared = pipeline.process(df, interval, context_df=context)
        
        # LÆ°u file
        out_path = data_dir / f"prepared_test_{interval}.csv"
        df_prepared.to_csv(out_path, index=False)
        logger.info(f"   ğŸ’¾ Saved: {out_path.name} (Cols: {len(df_prepared.columns)})")

if __name__ == "__main__":
    main()
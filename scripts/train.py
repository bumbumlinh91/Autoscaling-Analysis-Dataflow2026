"""
SCRIPT: TRAINING PIPELINE
--------------------------------------------
M√¥ t·∫£:
    K·ªãch b·∫£n hu·∫•n luy·ªán to√†n di·ªán cho h·ªá th·ªëng Autoscaling.
    T·ª± ƒë·ªông qu√©t c·∫•u h√¨nh, load d·ªØ li·ªáu ƒë√£ chu·∫©n b·ªã,
    hu·∫•n luy·ªán song song c√°c m√¥ h√¨nh (Prophet, XGBoost, LSTM)
    v√† xu·∫•t b√°o c√°o hi·ªáu nƒÉng chi ti·∫øt.
"""
import os
import sys
import torch
import yaml
import logging
import joblib
import pandas as pd
import numpy as np
from pathlib import Path
from sklearn.preprocessing import StandardScaler
from datetime import datetime

PROJECT_ROOT = Path(__file__).resolve().parent.parent
sys.path.append(str(PROJECT_ROOT))

# Import Models 
from src.models import ProphetForecaster, XGBoostForecaster, LSTMForecaster

# SETUP & CONFIG

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

def load_config():
    # D√πng ƒë∆∞·ªùng d·∫´n tuy·ªát ƒë·ªëi
    config_path = PROJECT_ROOT / "config/config.yaml"
    if config_path.exists():
        with open(config_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    logger.error(f"‚ùå Kh√¥ng t√¨m th·∫•y config t·∫°i: {config_path}")
    sys.exit(1)

CONFIG = load_config()


# TRAINER CLASS (MANAGER)

class DataflowTrainer:
    def __init__(self):
        self.config = CONFIG
        self.data_dir = PROJECT_ROOT / "data"
        self.models_dir = PROJECT_ROOT / "saved_models" 
        self.results_dir = PROJECT_ROOT / "results"
        
        self.models_dir.mkdir(parents=True, exist_ok=True)
        self.results_dir.mkdir(parents=True, exist_ok=True)
        
        self.target_col = 'intensity'

    def load_prepared_data(self, interval, mode='train'):
        """Load d·ªØ li·ªáu ƒë√£ qua Feature Engineering."""
        filename = f"prepared_{mode}_{interval}.csv"
        path = self.data_dir / filename
        
        if not path.exists():
            logger.warning(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file: {path}")
            return None
        
        df = pd.read_csv(path)
        if 'ds' in df.columns:
            df['ds'] = pd.to_datetime(df['ds'])
            # S·∫Øp x·∫øp theo th·ªùi gian ƒë·ªÉ ch·∫Øc ch·∫Øn
            df = df.sort_values('ds')
            
            # X√≥a d√≤ng tr√πng (gi·ªØ d√≤ng cu·ªëi c√πng ho·∫∑c ƒë·∫ßu ti√™n)
            # keep='last' ƒë·ªÉ ∆∞u ti√™n d·ªØ li·ªáu m·ªõi nh·∫•t n·∫øu c√≥ c·∫≠p nh·∫≠t
            initial_len = len(df)
            df = df.drop_duplicates(subset=['ds'], keep='last')
            
            if len(df) < initial_len:
                logger.warning(f"   üßπ ƒê√£ x√≥a {initial_len - len(df)} d√≤ng tr√πng l·∫∑p timestamp trong {filename}")
      

        # ƒê·∫£m b·∫£o kh√¥ng c√≤n NaN
        df = df.dropna()
        return df

    def train_interval(self, interval):
        """Hu·∫•n luy·ªán t·∫•t c·∫£ m√¥ h√¨nh cho m·ªôt khung th·ªùi gian c·ª• th·ªÉ (vd: 5min)."""
        logger.info(f"\n{'='*60}\n üöÄ B·∫ÆT ƒê·∫¶U HU·∫§N LUY·ªÜN INTERVAL: {interval}\n{'='*60}")
        
        # 1. Load Data
        df_train = self.load_prepared_data(interval, 'train')
        df_test = self.load_prepared_data(interval, 'test') 
        
        if df_train is None or df_test is None:
            return

        # T·ª± ƒë·ªông l·∫•y t·∫•t c·∫£ c√°c c·ªôt tr·ª´ c·ªôt th·ªùi gian v√† target
        exclude_cols = ['ds', 'timestamp', 'y', self.target_col]
        feature_cols = [c for c in df_train.columns if c not in exclude_cols]
        logger.info(f"   üîç Detected {len(feature_cols)} features: {feature_cols}")

        # 2. Chu·∫©n b·ªã d·ªØ li·ªáu Matrix (X, y)
        X_train = df_train[feature_cols].values
        y_train = df_train[self.target_col].values
        X_test = df_test[feature_cols].values
        y_test = df_test[self.target_col].values

        # 3. Scaling
        scaler_X = StandardScaler()
        scaler_y = StandardScaler()
        
        X_train_sc = scaler_X.fit_transform(X_train)
        y_train_sc = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()
        X_test_sc = scaler_X.transform(X_test)
        y_test_sc = scaler_y.transform(y_test.reshape(-1, 1)).flatten()
        
        # L∆∞u Scaler
        joblib.dump(scaler_X, self.models_dir / f"scaler_X_{interval}.pkl")
        joblib.dump(scaler_y, self.models_dir / f"scaler_y_{interval}.pkl")

        
        # MODEL 1: PROPHET
        
        if self.config['models']['prophet']['enabled']:
            model = ProphetForecaster(self.config)
            
            # T·∫°o b·∫£n sao ƒë·ªÉ kh√¥ng ·∫£nh h∆∞·ªüng d·ªØ li·ªáu g·ªëc
            pf_train = df_train.copy()
            
            # N·∫øu trong file c√≥ c·ªôt y (c·ªôt target c≈©), x√≥a n√≥ ƒëi
            if 'y' in pf_train.columns:
                pf_train = pf_train.drop(columns=['y'])
            
            # Rename c·ªôt cho ƒë√∫ng ƒë·ªãnh d·∫°ng Prophet
            pf_train = pf_train.rename(columns={'ds': 'ds', self.target_col: 'y'})
            
            # Reset index ƒë·ªÉ ƒë·∫£m b·∫£o an to√†n tuy·ªát ƒë·ªëi
            pf_train = pf_train.reset_index(drop=True)
            
            # Ch·ªçn Regressors
            pf_regressors = [c for c in feature_cols if c in pf_train.columns]
            
            model.fit(pf_train, regressors=pf_regressors)
            model.save(self.models_dir / f"prophet_{interval}.pkl")

       
        # MODEL 2: XGBOOST
        
        if self.config['models']['xgboost']['enabled']:
            model = XGBoostForecaster(self.config)
            # XGBoost d√πng d·ªØ li·ªáu ƒë√£ Scaling ho·∫∑c Raw ƒë·ªÅu ƒë∆∞·ª£c
            model.fit(X_train_sc, y_train_sc, X_test_sc, y_test_sc)
            
            # Eval s∆° b·ªô
            preds = model.predict(X_test_sc)
            metrics = model.evaluate(y_test_sc, preds)
            logger.info(f"üìä XGBoost Results ({interval}): {metrics}")
            
            model.save(self.models_dir / f"xgboost_{interval}.pkl")

        
        # MODEL 3: LSTM
       
        if self.config['models']['lstm']['enabled']:
            input_dim = X_train_sc.shape[1]
            model = LSTMForecaster(self.config, input_dim)      

            model.fit(X_train_sc, y_train_sc, X_test_sc, y_test_sc)
            
            # L∆∞u model Pytorch 
            torch.save(model.model.state_dict(), self.models_dir / f"lstm_{interval}.pth")
            logger.info(f"‚úÖ LSTM Model saved to lstm_{interval}.pth")

    def run_complete_training(self):
        """Ch·∫°y to√†n b·ªô pipeline cho m·ªçi interval."""
        intervals = self.config['processing']['intervals']
        for interval in intervals:
            try:
                self.train_interval(interval)
            except Exception as e:
                logger.error(f"‚ùå L·ªói khi train interval {interval}: {e}")
                import traceback
                traceback.print_exc()

# MAIN ENTRY

if __name__ == "__main__":
    trainer = DataflowTrainer()
    trainer.run_complete_training()
    
    print("\n" + "="*60)
    print("üéâ HU·∫§N LUY·ªÜN HO√ÄN T·∫§T! MODEL ƒê√É S·∫¥N S√ÄNG TRONG TH∆Ø M·ª§C 'saved_models/'")
    print("="*60)
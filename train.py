"""
MODULE: TRAINING PIPELINE (PRODUCTION READY)
--------------------------------------------
M√¥ t·∫£:
    K·ªãch b·∫£n hu·∫•n luy·ªán to√†n di·ªán cho h·ªá th·ªëng Autoscaling.
    T·ª± ƒë·ªông qu√©t c·∫•u h√¨nh, load d·ªØ li·ªáu ƒë√£ chu·∫©n b·ªã (prepared),
    hu·∫•n luy·ªán song song c√°c m√¥ h√¨nh (Prophet, XGBoost, LSTM)
    v√† xu·∫•t b√°o c√°o hi·ªáu nƒÉng chi ti·∫øt.

Quy tr√¨nh:
    1. Setup: Load Config, Logger, Device.
    2. Data Loading: ƒê·ªçc file prepared_{mode}_{interval}.csv.
    3. Feature Selection: T·ª± ƒë·ªông l·ªçc c·ªôt feature v√† target.
    4. Training Dispatcher: G·ªçi ƒë√∫ng class model t∆∞∆°ng ·ª©ng.
    5. Evaluation: T√≠nh RMSE, MAE.
    6. Reporting: L∆∞u k·∫øt qu·∫£ v√† model artifacts.

T√°c gi·∫£: Senior Data Scientist
"""
import os
import sys
import torch
import yaml
import logging
import joblib
import pandas as pd
import numpy as np
from pathlib import Path
from sklearn.preprocessing import StandardScaler
from datetime import datetime

# Import Models t·ª´ file models.py v·ª´a t·∫°o
from models import ProphetForecaster, XGBoostForecaster, LSTMForecaster

# ==============================================================================
# SETUP & CONFIG
# ==============================================================================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

def load_config():
    paths = [Path("config/config.yaml"), Path("../config/config.yaml")]
    for p in paths:
        if p.exists():
            return yaml.safe_load(open(p, 'r', encoding='utf-8'))
    logger.error("‚ùå Kh√¥ng t√¨m th·∫•y config.yaml")
    sys.exit(1)

CONFIG = load_config()

# ==============================================================================
# TRAINER CLASS (MANAGER)
# ==============================================================================
class DataflowTrainer:
    def __init__(self):
        self.config = CONFIG
        self.data_dir = Path(CONFIG['paths']['input_dir'])
        self.models_dir = Path("models")
        self.results_dir = Path("results")
        
        self.models_dir.mkdir(exist_ok=True)
        self.results_dir.mkdir(exist_ok=True)
        
        # Danh s√°ch Features (Ph·∫£i kh·ªõp v·ªõi feature_engineering.py)
        self.feature_cols = [
            'hour_sin', 'hour_cos', 'day_sin', 'day_cos', 'is_weekend',
            'lag_1step', 'lag_1h', 'lag_24h', 'lag_7d',
            'roll_mean_4h', 'roll_std_4h', 'roll_max_4h'
        ]
        self.target_col = 'intensity'

    def load_prepared_data(self, interval, mode='train'):
        """Load d·ªØ li·ªáu ƒë√£ qua Feature Engineering."""
        filename = f"prepared_{mode}_{interval}.csv"
        path = self.data_dir / filename
        
        if not path.exists():
            logger.warning(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file: {path}")
            return None
        
        df = pd.read_csv(path)
        if 'ds' in df.columns:
            # S·∫Øp x·∫øp theo th·ªùi gian ƒë·ªÉ ch·∫Øc ch·∫Øn
            df = df.sort_values('ds')
            
            # X√≥a d√≤ng tr√πng (gi·ªØ d√≤ng cu·ªëi c√πng ho·∫∑c ƒë·∫ßu ti√™n)
            # keep='last' ƒë·ªÉ ∆∞u ti√™n d·ªØ li·ªáu m·ªõi nh·∫•t n·∫øu c√≥ c·∫≠p nh·∫≠t
            initial_len = len(df)
            df = df.drop_duplicates(subset=['ds'], keep='last')
            
            if len(df) < initial_len:
                logger.warning(f"   üßπ ƒê√£ x√≥a {initial_len - len(df)} d√≤ng tr√πng l·∫∑p timestamp trong {filename}")
        # -------------------------------

        # ƒê·∫£m b·∫£o kh√¥ng c√≤n NaN
        df = df.dropna()
        return df

    def train_interval(self, interval):
        """Hu·∫•n luy·ªán t·∫•t c·∫£ m√¥ h√¨nh cho m·ªôt khung th·ªùi gian c·ª• th·ªÉ (vd: 5min)."""
        logger.info(f"\n{'='*60}\n üöÄ B·∫ÆT ƒê·∫¶U HU·∫§N LUY·ªÜN INTERVAL: {interval}\n{'='*60}")
        
        # 1. Load Data
        df_train = self.load_prepared_data(interval, 'train')
        df_test = self.load_prepared_data(interval, 'test') # D√πng l√†m Validation
        
        if df_train is None or df_test is None:
            return

        # 2. Chu·∫©n b·ªã d·ªØ li·ªáu Matrix (X, y)
        X_train = df_train[self.feature_cols].values
        y_train = df_train[self.target_col].values
        X_test = df_test[self.feature_cols].values
        y_test = df_test[self.target_col].values

        # 3. Scaling (Quan tr·ªçng cho LSTM/XGBoost)
        scaler_X = StandardScaler()
        scaler_y = StandardScaler()
        
        X_train_sc = scaler_X.fit_transform(X_train)
        y_train_sc = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()
        X_test_sc = scaler_X.transform(X_test)
        y_test_sc = scaler_y.transform(y_test.reshape(-1, 1)).flatten()
        
        # L∆∞u Scaler
        joblib.dump(scaler_X, self.models_dir / f"scaler_X_{interval}.pkl")
        joblib.dump(scaler_y, self.models_dir / f"scaler_y_{interval}.pkl")

        # ==================================================
        # MODEL 1: PROPHET
        # ==================================================
        if self.config['models']['prophet']['enabled']:
            model = ProphetForecaster(self.config)
            
            # [FIX QUAN TR·ªåNG] X·ª≠ l√Ω tr√πng l·∫∑p c·ªôt 'y'
            # T·∫°o b·∫£n sao ƒë·ªÉ kh√¥ng ·∫£nh h∆∞·ªüng d·ªØ li·ªáu g·ªëc
            pf_train = df_train.copy()
            
            # N·∫øu trong file csv ƒë√£ c√≥ s·∫µn c·ªôt 'y' (r√°c t·ª´ b∆∞·ªõc tr∆∞·ªõc), ph·∫£i x√≥a n√≥ ƒëi!
            if 'y' in pf_train.columns:
                pf_train = pf_train.drop(columns=['y'])
            
            # Gi·ªù m·ªõi ƒë∆∞·ª£c rename 'intensity' -> 'y'
            pf_train = pf_train.rename(columns={'ds': 'ds', self.target_col: 'y'})
            
            # Reset index ƒë·ªÉ ƒë·∫£m b·∫£o an to√†n tuy·ªát ƒë·ªëi
            pf_train = pf_train.reset_index(drop=True)
            
            # Ch·ªçn Regressors
            pf_regressors = [c for c in self.feature_cols if c in pf_train.columns]
            
            model.fit(pf_train, regressors=pf_regressors)
            model.save(self.models_dir / f"prophet_{interval}.pkl")

        # ==================================================
        # MODEL 2: XGBOOST
        # ==================================================
        if self.config['models']['xgboost']['enabled']:
            model = XGBoostForecaster(self.config)
            # XGBoost d√πng d·ªØ li·ªáu ƒë√£ Scaling ho·∫∑c Raw ƒë·ªÅu ƒë∆∞·ª£c (Scaling t·ªët h∆°n ch√∫t)
            model.fit(X_train_sc, y_train_sc, X_test_sc, y_test_sc)
            
            # Eval s∆° b·ªô
            preds = model.predict(X_test_sc)
            metrics = model.evaluate(y_test_sc, preds)
            logger.info(f"üìä XGBoost Results ({interval}): {metrics}")
            
            model.save(self.models_dir / f"xgboost_{interval}.pkl")

        # ==================================================
        # MODEL 3: LSTM
        # ==================================================
        if self.config['models']['lstm']['enabled']:
            input_dim = X_train_sc.shape[1]
            model = LSTMForecaster(self.config, input_dim)      

            model.fit(X_train_sc, y_train_sc, X_test_sc, y_test_sc)
            
            # L∆∞u model Pytorch (ch·ªâ l∆∞u state_dict cho nh·∫π)
            torch.save(model.model.state_dict(), self.models_dir / f"lstm_{interval}.pth")
            logger.info(f"‚úÖ LSTM Model saved to lstm_{interval}.pth")

    def run_complete_training(self):
        """Ch·∫°y to√†n b·ªô pipeline cho m·ªçi interval."""
        intervals = self.config['processing']['intervals']
        for interval in intervals:
            try:
                self.train_interval(interval)
            except Exception as e:
                logger.error(f"‚ùå L·ªói khi train interval {interval}: {e}")
                import traceback
                traceback.print_exc()

# ==============================================================================
# MAIN ENTRY
# ==============================================================================
if __name__ == "__main__":
    trainer = DataflowTrainer()
    trainer.run_complete_training()
    
    print("\n" + "="*60)
    print("üéâ HU·∫§N LUY·ªÜN HO√ÄN T·∫§T! MODEL ƒê√É S·∫¥N S√ÄNG TRONG TH∆Ø M·ª§C 'models/'")
    print("="*60)